{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2eecb96-cf0e-47ed-8116-88a7eaa4236d",
   "metadata": {},
   "source": "# 교차 스레드 지속성을 추가하는 방법 (함수형 API)\n\n!!! info \"필수 조건\"\n\n    이 가이드는 다음에 대한 익숙함을 가정합니다:\n    \n    - [함수형 API](../../concepts/functional_api/)\n    - [지속성](../../concepts/persistence/)\n    - [메모리](../../concepts/memory/)\n    - [채팅 모델](https://python.langchain.com/docs/concepts/chat_models/)\n\nLangGraph를 사용하면 **다른 [스레드](../../concepts/persistence/#threads)** 간에 데이터를 유지할 수 있습니다. 예를 들어, 사용자에 대한 정보(이름이나 선호도)를 공유(교차 스레드) 메모리에 저장하고 새 스레드(예: 새 대화)에서 재사용할 수 있습니다.\n\n[함수형 API](../../concepts/functional_api/)를 사용할 때 [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) 인터페이스를 사용하여 메모리를 저장하고 검색하도록 설정할 수 있습니다:\n\n1. `Store`의 인스턴스를 생성합니다\n\n    ```python\n    from langgraph.store.memory import InMemoryStore, BaseStore\n    \n    store = InMemoryStore()\n    ```\n\n2. `store` 인스턴스를 `entrypoint()` 데코레이터에 전달하고 함수 시그니처에서 `store` 매개변수를 노출합니다:\n\n    ```python\n    from langgraph.func import entrypoint\n\n    @entrypoint(store=store)\n    def workflow(inputs: dict, store: BaseStore):\n        my_task(inputs).result()\n        ...\n    ```\n    \n이 가이드에서는 [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) 인터페이스를 사용하여 구현된 공유 메모리가 있는 워크플로우를 구성하고 사용하는 방법을 보여줍니다.\n\n!!! note 참고\n\n    이 가이드에서 사용되는 [`Store`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) API에 대한 지원은 LangGraph `v0.2.32`에 추가되었습니다.\n\n    이 가이드에서 사용되는 [`Store`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) API의 __index__ 및 __query__ 인수에 대한 지원은 LangGraph `v0.2.54`에 추가되었습니다.\n\n!!! tip \"참고\"\n\n    `StateGraph`에 교차 스레드 지속성을 추가해야 하는 경우 이 [how-to 가이드](../cross-thread-persistence)를 확인하세요.\n\n## 설정\n\n먼저 필요한 패키지를 설치하고 API 키를 설정하겠습니다"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3457aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_anthropic langchain_openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c64a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6817d",
   "metadata": {},
   "source": "!!! tip \"LangGraph 개발을 위한 [LangSmith](https://smith.langchain.com) 설정\"\n\n    LangSmith에 가입하여 문제를 빠르게 발견하고 LangGraph 프로젝트의 성능을 개선하세요. LangSmith를 사용하면 LangGraph로 구축한 LLM 앱을 디버그, 테스트 및 모니터링하기 위해 추적 데이터를 사용할 수 있습니다 — 시작하는 방법에 대해 자세히 알아보려면 [여기](https://docs.smith.langchain.com)를 참조하세요"
  },
  {
   "cell_type": "markdown",
   "id": "6b5b3d42-3d2c-455e-ac10-e2ae74dc1cf1",
   "metadata": {},
   "source": "## 예제: 장기 메모리가 있는 간단한 챗봇"
  },
  {
   "cell_type": "markdown",
   "id": "c4c550b5-1954-496b-8b9d-800361af17dc",
   "metadata": {},
   "source": "### 저장소 정의\n\n이 예제에서는 사용자의 선호도에 대한 정보를 검색할 수 있는 워크플로우를 만듭니다. `InMemoryStore`를 정의하여 이를 수행합니다 - 메모리에 데이터를 저장하고 해당 데이터를 쿼리할 수 있는 객체입니다.\n\n`Store` 인터페이스를 사용하여 객체를 저장할 때 두 가지를 정의합니다:\n\n* 객체의 네임스페이스, 튜플(디렉토리와 유사)\n* 객체 키(파일 이름과 유사)\n\n우리 예제에서는 네임스페이스로 `(\"memories\", <user_id>)`를 사용하고 각 새 메모리의 키로 임의의 UUID를 사용합니다.\n\n중요한 것은 사용자를 결정하기 위해 노드 함수의 config 키워드 인수를 통해 `user_id`를 전달한다는 것입니다.\n\n먼저 저장소를 정의해 봅시다!"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f303d6-612e-4e34-bf36-29d4ed25d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "in_memory_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "        \"dims\": 1536,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389c9f4-226d-40c7-8bfc-ee8aac24f79d",
   "metadata": {},
   "source": "### 워크플로우 생성"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30a362-528c-45ee-9df6-630d2d843588",
   "metadata": {},
   "outputs": [],
   "source": "import uuid\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.func import entrypoint, task\nfrom langgraph.graph import add_messages\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.store.base import BaseStore\n\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n\n@task\ndef call_model(messages: list[BaseMessage], memory_store: BaseStore, user_id: str):\n    namespace = (\"memories\", user_id)\n    last_message = messages[-1]\n    memories = memory_store.search(namespace, query=str(last_message.content))\n    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n    # 사용자가 모델에게 기억하라고 요청하면 새 메모리를 저장합니다\n    if \"remember\" in last_message.content.lower():\n        memory = \"User name is Bob\"\n        memory_store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n\n    response = model.invoke([{\"role\": \"system\", \"content\": system_msg}] + messages)\n    return response\n\n\n# 참고: entrypoint()를 통해 워크플로우를 생성할 때 여기에 store 객체를 전달합니다\n@entrypoint(checkpointer=InMemorySaver(), store=in_memory_store)\ndef workflow(\n    inputs: list[BaseMessage],\n    *,\n    previous: list[BaseMessage],\n    config: RunnableConfig,\n    store: BaseStore,\n):\n    user_id = config[\"configurable\"][\"user_id\"]\n    previous = previous or []\n    inputs = add_messages(previous, inputs)\n    response = call_model(inputs, store, user_id).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))"
  },
  {
   "cell_type": "markdown",
   "id": "f22a4a18-67e4-4f0b-b655-a29bbe202e1c",
   "metadata": {},
   "source": "!!! note 참고\n\n    LangGraph Cloud 또는 LangGraph Studio를 사용하는 경우 자동으로 수행되므로 entrypoint 데코레이터에 store를 전달할 __필요가 없습니다__."
  },
  {
   "cell_type": "markdown",
   "id": "552d4e33-556d-4fa5-8094-2a076bc21529",
   "metadata": {},
   "source": "### 워크플로우 실행!"
  },
  {
   "cell_type": "markdown",
   "id": "1842c626-6cd9-4f58-b549-58978e478098",
   "metadata": {},
   "source": "이제 config에서 사용자 ID를 지정하고 모델에 이름을 알려주겠습니다:"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c871a073-a466-46ad-aafe-2b870831057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! Nice to meet you. I'll remember that your name is Bob. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "input_message = {\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}\n",
    "for chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n",
    "    chunk.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d862be40-1f8a-4057-81c4-b7bf073dc4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "input_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\n",
    "for chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n",
    "    chunk.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd01ec-f135-4811-8743-daff8daea422",
   "metadata": {},
   "source": "이제 인메모리 저장소를 검사하여 사용자에 대한 메모리를 실제로 저장했는지 확인할 수 있습니다:"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76cde493-89cf-4709-a339-207d2b7e9ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 'User name is Bob'}\n"
     ]
    }
   ],
   "source": [
    "for memory in in_memory_store.search((\"memories\", \"1\")):\n",
    "    print(memory.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5d7eb-af23-4131-b8fd-2a69e74e6e55",
   "metadata": {},
   "source": "이제 다른 사용자에 대해 워크플로우를 실행하여 첫 번째 사용자에 대한 메모리가 자체 포함되어 있는지 확인해 봅시다:"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d362350b-d730-48bd-9652-983812fd7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have any information about your name. I can only see our current conversation without any prior context or personal details about you. If you'd like me to know your name, feel free to tell me!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"2\"}}\n",
    "input_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\n",
    "for chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n",
    "    chunk.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}