{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 처음부터 ReAct 에이전트를 만드는 방법 (함수형 API)\n\n!!! info \"필수 조건\"\n    이 가이드는 다음에 대한 익숙함을 가정합니다:\n    \n    - [채팅 모델](https://python.langchain.com/docs/concepts/chat_models)\n    - [메시지](https://python.langchain.com/docs/concepts/messages)\n    - [도구 호출](https://python.langchain.com/docs/concepts/tool_calling/)\n    - [엔트리포인트](../../concepts/functional_api/#entrypoint) 및 [태스크](../../concepts/functional_api/#task)\n\n이 가이드는 LangGraph [함수형 API](../../concepts/functional_api)를 사용하여 ReAct 에이전트를 구현하는 방법을 보여줍니다.\n\nReAct 에이전트는 다음과 같이 작동하는 [도구 호출 에이전트](../../concepts/agentic_concepts/#tool-calling-agent)입니다:\n\n1. 쿼리가 채팅 모델에 발행됩니다;\n2. 모델이 [도구 호출](../../concepts/agentic_concepts/#tool-calling)을 생성하지 않으면 모델 응답을 반환합니다.\n3. 모델이 도구 호출을 생성하면 사용 가능한 도구로 도구 호출을 실행하고, [도구 메시지](https://python.langchain.com/docs/concepts/messages/)로 메시지 목록에 추가하고, 프로세스를 반복합니다.\n\n이것은 메모리, 사람-인-더-루프 기능 및 기타 기능으로 확장할 수 있는 간단하고 다재다능한 설정입니다. 예제는 전용 [how-to 가이드](../../how-tos/#prebuilt-react-agent)를 참조하세요.\n\n## 설정\n\n먼저 필요한 패키지를 설치하고 API 키를 설정하겠습니다:"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<div class=\"admonition tip\">\n     <p class=\"admonition-title\">더 나은 디버깅을 위한 <a href=\"https://smith.langchain.com\">LangSmith</a> 설정</p>\n     <p style=\"padding-top: 5px;\">\n         LangSmith에 가입하여 문제를 빠르게 발견하고 LangGraph 프로젝트의 성능을 개선하세요. LangSmith를 사용하면 LangGraph로 구축한 LLM 앱을 디버그, 테스트 및 모니터링하기 위해 추적 데이터를 사용할 수 있습니다 — 시작하는 방법에 대해 자세히 알아보려면 <a href=\"https://docs.smith.langchain.com\">문서</a>를 참조하세요. \n     </p>\n </div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ReAct 에이전트 생성\n\n이제 필요한 패키지를 설치하고 환경 변수를 설정했으므로 에이전트를 만들 수 있습니다.\n\n### 모델 및 도구 정의\n\n먼저 예제에 사용할 도구와 모델을 정의하겠습니다. 여기서는 위치의 날씨 설명을 가져오는 단일 플레이스홀더 도구를 사용합니다.\n\n이 예제에서는 [OpenAI](https://python.langchain.com/docs/integrations/providers/openai/) 채팅 모델을 사용하지만 [도구 호출을 지원하는](https://python.langchain.com/docs/integrations/chat/) 모든 모델이 충분합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\n@tool\ndef get_weather(location: str):\n    \"\"\"특정 위치의 날씨를 가져오기 위해 호출합니다.\"\"\"\n    # 이것은 실제 구현을 위한 플레이스홀더입니다\n    if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n        return \"It's sunny!\"\n    elif \"boston\" in location.lower():\n        return \"It's rainy!\"\n    else:\n        return f\"I am not sure what the weather is in {location}\"\n\n\ntools = [get_weather]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 태스크 정의\n\n다음으로 실행할 [태스크](../../concepts/functional_api/#task)를 정의합니다. 여기에는 두 가지 다른 태스크가 있습니다:\n\n1. **모델 호출**: 메시지 목록으로 채팅 모델을 쿼리하려고 합니다.\n2. **도구 호출**: 모델이 도구 호출을 생성하면 실행하려고 합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.messages import ToolMessage\nfrom langgraph.func import entrypoint, task\n\ntools_by_name = {tool.name: tool for tool in tools}\n\n\n@task\ndef call_model(messages):\n    \"\"\"메시지 시퀀스로 모델을 호출합니다.\"\"\"\n    response = model.bind_tools(tools).invoke(messages)\n    return response\n\n\n@task\ndef call_tool(tool_call):\n    tool = tools_by_name[tool_call[\"name\"]]\n    observation = tool.invoke(tool_call[\"args\"])\n    return ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 엔트리포인트 정의\n\n우리의 [엔트리포인트](../../concepts/functional_api/#entrypoint)는 이 두 태스크의 오케스트레이션을 처리할 것입니다. 위에서 설명한 대로 `call_model` 태스크가 도구 호출을 생성하면 `call_tool` 태스크가 각각에 대한 응답을 생성합니다. 모든 메시지를 단일 메시지 목록에 추가합니다.\n\n!!! tip\n    태스크가 future와 같은 객체를 반환하기 때문에 아래 구현은 도구를 병렬로 실행합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langgraph.graph.message import add_messages\n\n\n@entrypoint()\ndef agent(messages):\n    llm_response = call_model(messages).result()\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # 도구 실행\n        tool_result_futures = [\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\n        ]\n        tool_results = [fut.result() for fut in tool_result_futures]\n\n        # 메시지 목록에 추가\n        messages = add_messages(messages, [llm_response, *tool_results])\n\n        # 모델 다시 호출\n        llm_response = call_model(messages).result()\n\n    return llm_response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 사용\n\n에이전트를 사용하려면 메시지 목록으로 호출합니다. 구현에 따라 LangChain [메시지](https://python.langchain.com/docs/concepts/messages/) 객체 또는 OpenAI 스타일 딕셔너리가 될 수 있습니다:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "user_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\nprint(user_message)\n\nfor step in agent.stream([user_message]):\n    for task_name, message in step.items():\n        if task_name == \"agent\":\n            continue  # 태스크 업데이트만 출력\n        print(f\"\\n{task_name}:\")\n        message.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "완벽합니다! 그래프가 `get_weather` 도구를 올바르게 호출하고 도구에서 정보를 받은 후 사용자에게 응답합니다. LangSmith 추적을 [여기](https://smith.langchain.com/public/d5a0d5ea-bdaa-4032-911e-7db177c8141b/r)에서 확인하세요."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 스레드 수준 지속성 추가\n\n[스레드 수준 지속성](../../concepts/persistence#threads)을 추가하면 에이전트와의 대화 경험을 지원할 수 있습니다: 후속 호출은 이전 메시지 목록에 추가되어 전체 대화 컨텍스트를 유지합니다.\n\n에이전트에 스레드 수준 지속성을 추가하려면:\n\n1. [체크포인터](../../concepts/persistence#checkpointer-libraries) 선택: 여기서는 간단한 인메모리 체크포인터인 [InMemorySaver](../../reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)를 사용합니다.\n2. 이전 메시지 상태를 두 번째 인수로 받도록 엔트리포인트를 업데이트합니다. 여기서는 메시지 업데이트를 이전 메시지 시퀀스에 단순히 추가합니다.\n3. 워크플로우에서 반환될 값과 `entrypoint.final`을 사용하여 체크포인터에 의해 `previous`로 저장될 값을 선택합니다 (선택 사항)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langgraph.checkpoint.memory import InMemorySaver\n\n# highlight-next-line\ncheckpointer = InMemorySaver()\n\n\n# highlight-next-line\n@entrypoint(checkpointer=checkpointer)\n# highlight-next-line\ndef agent(messages, previous):\n    # highlight-next-line\n    if previous is not None:\n        # highlight-next-line\n        messages = add_messages(previous, messages)\n\n    llm_response = call_model(messages).result()\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # 도구 실행\n        tool_result_futures = [\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\n        ]\n        tool_results = [fut.result() for fut in tool_result_futures]\n\n        # 메시지 목록에 추가\n        messages = add_messages(messages, [llm_response, *tool_results])\n\n        # 모델 다시 호출\n        llm_response = call_model(messages).result()\n\n    # 최종 응답 생성\n    messages = add_messages(messages, llm_response)\n    # highlight-next-line\n    return entrypoint.final(value=llm_response, save=messages)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "이제 애플리케이션을 실행할 때 config를 전달해야 합니다. config는 대화 스레드의 식별자를 지정합니다.\n\n!!! tip\n\n    스레드 수준 지속성에 대해 자세히 알아보려면 [개념 페이지](../../concepts/persistence/)와 [how-to 가이드](../../how-tos/#persistence)를 참조하세요."
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "이전과 같은 방식으로 스레드를 시작하지만 이번에는 config를 전달합니다:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "user_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\nprint(user_message)\n\n# highlight-next-line\nfor step in agent.stream([user_message], config):\n    for task_name, message in step.items():\n        if task_name == \"agent\":\n            continue  # 태스크 업데이트만 출력\n        print(f\"\\n{task_name}:\")\n        message.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "후속 대화를 물어보면 모델은 이전 컨텍스트를 사용하여 날씨에 대해 묻고 있음을 추론합니다:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "user_message = {\"role\": \"user\", \"content\": \"How does it compare to Boston, MA?\"}\nprint(user_message)\n\nfor step in agent.stream([user_message], config):\n    for task_name, message in step.items():\n        if task_name == \"agent\":\n            continue  # 태스크 업데이트만 출력\n        print(f\"\\n{task_name}:\")\n        message.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[LangSmith 추적](https://smith.langchain.com/public/20a1116b-bb3b-44c1-8765-7a28663439d9/r)에서 각 모델 호출에서 전체 대화 컨텍스트가 유지되는 것을 확인할 수 있습니다."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}