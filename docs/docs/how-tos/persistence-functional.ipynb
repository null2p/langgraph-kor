{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": "# 스레드 수준 지속성을 추가하는 방법 (함수형 API)\n\n!!! info \"필수 조건\"\n\n    이 가이드는 다음에 대한 익숙함을 가정합니다:\n    \n    - [함수형 API](../../concepts/functional_api/)\n    - [지속성](../../concepts/persistence/)\n    - [메모리](../../concepts/memory/)\n    - [채팅 모델](https://python.langchain.com/docs/concepts/chat_models/)\n\n!!! info \"LangGraph API 사용자에게는 필요하지 않음\"\n\n    LangGraph API를 사용하는 경우 체크포인터를 수동으로 구현할 필요가 없습니다. API가 자동으로 체크포인팅을 처리합니다. 이 가이드는 자체 커스텀 서버에서 LangGraph를 구현할 때 관련이 있습니다.\n\n많은 AI 애플리케이션은 동일한 [스레드](../../concepts/persistence#threads)에서 여러 상호 작용 간에 컨텍스트를 공유하기 위해 메모리가 필요합니다(예: 대화의 여러 턴). LangGraph 함수형 API에서 이러한 종류의 메모리는 [스레드 수준 지속성](https://langchain-ai.github.io/langgraph/concepts/persistence)을 사용하여 모든 [entrypoint()][langgraph.func.entrypoint] 워크플로우에 추가할 수 있습니다.\n\nLangGraph 워크플로우를 생성할 때 [체크포인터](https://langchain-ai.github.io/langgraph/reference/checkpoints/#basecheckpointsaver)를 사용하여 결과를 유지하도록 설정할 수 있습니다:\n\n\n1. 체크포인터의 인스턴스를 생성합니다:\n\n    ```python\n    from langgraph.checkpoint.memory import InMemorySaver\n    \n    checkpointer = InMemorySaver()       \n    ```\n\n2. `checkpointer` 인스턴스를 `entrypoint()` 데코레이터에 전달합니다:\n\n    ```python\n    from langgraph.func import entrypoint\n    \n    @entrypoint(checkpointer=checkpointer)\n    def workflow(inputs)\n        ...\n    ```\n\n3. 선택적으로 워크플로우 함수 시그니처에서 `previous` 매개변수를 노출합니다:\n\n    ```python\n    @entrypoint(checkpointer=checkpointer)\n    def workflow(\n        inputs,\n        *,\n        # 선택적으로 워크플로우 함수 시그니처에서 `previous`를 지정하여\n        # 마지막 실행 시점의 워크플로우에서 반환된 값에 액세스할 수 있습니다\n        previous\n    ):\n        previous = previous or []\n        combined_inputs = previous + inputs\n        result = do_something(combined_inputs)\n        ...\n    ```\n\n4. 선택적으로 워크플로우에서 반환할 값과 체크포인터가 `previous`로 저장할 값을 선택합니다:\n\n    ```python\n    @entrypoint(checkpointer=checkpointer)\n    def workflow(inputs, *, previous):\n        ...\n        result = do_something(...)\n        return entrypoint.final(value=result, save=combine(inputs, result))\n    ```\n\n이 가이드는 워크플로우에 스레드 수준 지속성을 추가하는 방법을 보여줍니다.\n\n!!! tip \"참고\"\n\n    여러 대화 또는 사용자 간에 __공유__되는 메모리가 필요한 경우(교차 스레드 지속성) 이 [how-to 가이드](../cross-thread-persistence-functional)를 확인하세요.\n\n!!! tip \"참고\"\n\n    `StateGraph`에 스레드 수준 지속성을 추가해야 하는 경우 이 [how-to 가이드](../persistence)를 확인하세요."
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": "## 설정\n\n먼저 필요한 패키지를 설치해야 합니다"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe11f4-62ed-4dc4-8875-3db21e260d1d",
   "metadata": {},
   "source": "다음으로 Anthropic(우리가 사용할 LLM)에 대한 API 키를 설정해야 합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c",
   "metadata": {},
   "source": "<div class=\"admonition tip\">\n    <p class=\"admonition-title\">LangGraph 개발을 위한 <a href=\"https://smith.langchain.com\">LangSmith</a> 설정</p>\n    <p style=\"padding-top: 5px;\">\n        LangSmith에 가입하여 문제를 빠르게 발견하고 LangGraph 프로젝트의 성능을 개선하세요. LangSmith를 사용하면 LangGraph로 구축한 LLM 앱을 디버그, 테스트 및 모니터링하기 위해 추적 데이터를 사용할 수 있습니다 — 시작하는 방법에 대해 자세히 알아보려면 <a href=\"https://docs.smith.langchain.com\">여기</a>를 참조하세요. \n    </p>\n</div>"
  },
  {
   "cell_type": "markdown",
   "id": "4cf509bc",
   "metadata": {},
   "source": "## 예제: 단기 메모리가 있는 간단한 챗봇\n\n[채팅 모델](https://python.langchain.com/docs/concepts/chat_models/)을 호출하는 단일 작업이 있는 워크플로우를 사용할 것입니다.\n\n먼저 사용할 모델을 정의해 봅시다:"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892b54b9-75f0-4804-9ed0-88b5e5532989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a2792-982b-4e47-83eb-0c594725d1c1",
   "metadata": {},
   "source": "이제 작업과 워크플로우를 정의할 수 있습니다. 지속성을 추가하려면 [Checkpointer](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)를 [entrypoint()][langgraph.func.entrypoint] 데코레이터에 전달해야 합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87326ea6-34c5-46da-a41f-dda26ef9bd74",
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.messages import BaseMessage\nfrom langgraph.graph import add_messages\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\n@task\ndef call_model(messages: list[BaseMessage]):\n    response = model.invoke(messages)\n    return response\n\n\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage]):\n    if previous:\n        inputs = add_messages(previous, inputs)\n\n    response = call_model(inputs).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))"
  },
  {
   "cell_type": "markdown",
   "id": "250d8fd9-2e7a-4892-9adc-19762a1e3cce",
   "metadata": {},
   "source": "이 워크플로우를 사용하려고 하면 대화의 컨텍스트가 상호 작용 간에 유지됩니다:"
  },
  {
   "cell_type": "markdown",
   "id": "7654ebcc-2179-41b4-92d1-6666f6f8634f",
   "metadata": {},
   "source": "!!! note 참고\n\n    LangGraph Platform 또는 LangGraph Studio를 사용하는 경우 자동으로 수행되므로 entrypoint 데코레이터에 체크포인터를 전달할 __필요가 없습니다__."
  },
  {
   "cell_type": "markdown",
   "id": "2a1b56c5-bd61-4192-8bdb-458a1e9f0159",
   "metadata": {},
   "source": "이제 에이전트와 상호 작용하면 이전 메시지를 기억하는 것을 볼 수 있습니다!"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd140f0-a5a6-4697-8115-322242f197b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! I'm Claude. Nice to meet you! How are you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = {\"role\": \"user\", \"content\": \"hi! I'm bob\"}\n",
    "for chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n",
    "    chunk.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb07bf8-68b7-4049-a0f1-eb67a4879a3a",
   "metadata": {},
   "source": "항상 이전 스레드를 재개할 수 있습니다:"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ae8246-11d5-40e1-8567-361e5bef8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "input_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\n",
    "for chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n",
    "    chunk.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47bbfc-d9ef-4288-ba4a-ebbc0136fa9d",
   "metadata": {},
   "source": "새 대화를 시작하려면 다른 `thread_id`를 전달할 수 있습니다. 짠! 모든 메모리가 사라졌습니다!"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "273d56a8-f40f-4a51-a27f-7c6bb2bda0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name unless you tell me. Each conversation I have starts fresh, so I don't have access to any previous interactions or personal information unless you share it with me.\n"
     ]
    }
   ],
   "source": [
    "input_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\n",
    "for chunk in workflow.stream(\n",
    "    [input_message],\n",
    "    {\"configurable\": {\"thread_id\": \"2\"}},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7926a8-4c88-4b16-973c-53d6da3f4a08",
   "metadata": {},
   "source": "!!! tip \"토큰 스트리밍\"\n\n    챗봇에서 LLM 토큰을 스트리밍하려면 `stream_mode=\"messages\"`를 사용할 수 있습니다. 자세히 알아보려면 이 [how-to 가이드](../streaming-tokens)를 확인하세요."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}