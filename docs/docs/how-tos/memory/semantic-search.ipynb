{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 에이전트 메모리에 의미론적 검색을 추가하는 방법\n\n이 가이드는 에이전트의 메모리 저장소에서 의미론적 검색을 활성화하는 방법을 보여줍니다. 이를 통해 의미론적 유사성으로 저장소의 항목을 검색할 수 있습니다.\n\n!!! tip 필수 조건\n    이 가이드는 [LangGraph의 메모리](https://langchain-ai.github.io/langgraph/concepts/memory/)에 대한 익숙함을 가정합니다.\n\n먼저 이 가이드의 필수 조건을 설치합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "다음으로 [인덱스 구성](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.IndexConfig)으로 저장소를 만듭니다. 기본적으로 저장소는 의미론적/벡터 검색 없이 구성됩니다. 저장소를 생성할 때 [IndexConfig](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.IndexConfig)를 저장소의 생성자에 제공하여 항목 인덱싱을 선택할 수 있습니다. 저장소 클래스가 이 인터페이스를 구현하지 않거나 인덱스 구성을 전달하지 않으면 의미론적 검색이 비활성화되고 `put` 또는 `aput`에 전달된 모든 `index` 인수는 효과가 없습니다. 아래는 예제입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain.embeddings import init_embeddings\nfrom langgraph.store.memory import InMemoryStore\n\n# 의미론적 검색이 활성화된 저장소 생성\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "이제 몇 가지 메모리를 저장해 봅시다:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 일부 메모리 저장\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I prefer Italian food\"})\nstore.put((\"user_123\", \"memories\"), \"3\", {\"text\": \"I don't like spicy food\"})\nstore.put((\"user_123\", \"memories\"), \"3\", {\"text\": \"I am studying econometrics\"})\nstore.put((\"user_123\", \"memories\"), \"3\", {\"text\": \"I am a plumber\"})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "자연어를 사용하여 메모리를 검색합니다:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 음식 선호도에 대한 메모리 찾기\nmemories = store.search((\"user_123\", \"memories\"), query=\"I like food?\", limit=5)\n\nfor memory in memories:\n    print(f\"Memory: {memory.value['text']} (similarity: {memory.score})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 에이전트에서 사용하기\n\n저장소를 주입하여 모든 노드에 의미론적 검색을 추가합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import Optional\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.store.base import BaseStore\n\nfrom langgraph.graph import START, MessagesState, StateGraph\n\nllm = init_chat_model(\"openai:gpt-4o-mini\")\n\n\ndef chat(state, *, store: BaseStore):\n    # 사용자의 마지막 메시지를 기반으로 검색\n    items = store.search(\n        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n    )\n    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n    response = llm.invoke(\n        [\n            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n            *state[\"messages\"],\n        ]\n    )\n    return {\"messages\": [response]}\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(chat)\nbuilder.add_edge(START, \"chat\")\ngraph = builder.compile(store=store)\n\nfor message, metadata in graph.stream(\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n    stream_mode=\"messages\",\n):\n    print(message.content, end=\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## `create_react_agent`에서 사용하기 {#using-in-create-react-agent}\n\n`prompt` 함수에 저장소를 주입하여 도구 호출 에이전트에 의미론적 검색을 추가합니다. 에이전트가 수동으로 메모리를 저장하거나 검색할 수 있도록 도구에서 저장소를 사용할 수도 있습니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import uuid\nfrom typing import Optional\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.prebuilt import InjectedStore\nfrom langgraph.store.base import BaseStore\nfrom typing_extensions import Annotated\n\nfrom langgraph.prebuilt import create_react_agent\n\n\ndef prepare_messages(state, *, store: BaseStore):\n    # 사용자의 마지막 메시지를 기반으로 검색\n    items = store.search(\n        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n    )\n    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n    return [\n        {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"}\n    ] + state[\"messages\"]\n\n\n# 도구 내에서 저장소를 직접 사용할 수도 있습니다!\ndef upsert_memory(\n    content: str,\n    *,\n    memory_id: Optional[uuid.UUID] = None,\n    store: Annotated[BaseStore, InjectedStore],\n):\n    \"\"\"데이터베이스에 메모리를 업서트합니다.\"\"\"\n    # LLM은 이 도구를 사용하여 새 메모리를 저장할 수 있습니다\n    mem_id = memory_id or uuid.uuid4()\n    store.put(\n        (\"user_123\", \"memories\"),\n        key=str(mem_id),\n        value={\"text\": content},\n    )\n    return f\"Stored memory {mem_id}\"\n\n\nagent = create_react_agent(\n    init_chat_model(\"openai:gpt-4o-mini\"),\n    tools=[upsert_memory],\n    # 'prompt' 함수는 LLM용 메시지를 준비하기 위해 실행됩니다. 각 LLM 호출 직전에 호출됩니다\n    # 각 LLM 호출 직전에 호출됩니다\n    prompt=prepare_messages,\n    store=store,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are you in the mood for? Since you love Italian food and pizza, maybe something in that realm would be great! Would you like suggestions for a specific dish or restaurant?"
     ]
    }
   ],
   "source": [
    "for message, metadata in agent.stream(\n",
    "    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 고급 사용법\n\n#### 다중 벡터 인덱싱\n\n메모리의 다양한 측면을 개별적으로 저장하고 검색하여 리콜을 개선하거나 특정 필드를 인덱싱에서 제외합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 메모리 내용과 감정적 맥락을 모두 임베딩하도록 저장소 구성\nstore = InMemoryStore(\n    index={\"embed\": embeddings, \"dims\": 1536, \"fields\": [\"memory\", \"emotional_context\"]}\n)\n# 다양한 내용/감정 쌍으로 메모리 저장\nstore.put(\n    (\"user_123\", \"memories\"),\n    \"mem1\",\n    {\n        \"memory\": \"Had pizza with friends at Mario's\",\n        \"emotional_context\": \"felt happy and connected\",\n        \"this_isnt_indexed\": \"I prefer ravioli though\",\n    },\n)\nstore.put(\n    (\"user_123\", \"memories\"),\n    \"mem2\",\n    {\n        \"memory\": \"Ate alone at home\",\n        \"emotional_context\": \"felt a bit lonely\",\n        \"this_isnt_indexed\": \"I like pie\",\n    },\n)\n\n# 감정 상태에 초점을 맞춘 검색 - mem2와 일치\nresults = store.search(\n    (\"user_123\", \"memories\"), query=\"times they felt isolated\", limit=1\n)\nprint(\"Expect mem 2\")\nfor r in results:\n    print(f\"Item: {r.key}; Score ({r.score})\")\n    print(f\"Memory: {r.value['memory']}\")\n    print(f\"Emotion: {r.value['emotional_context']}\\n\")\n\n# 사교적 식사에 초점을 맞춘 검색 - mem1과 일치\nprint(\"Expect mem1\")\nresults = store.search((\"user_123\", \"memories\"), query=\"fun pizza\", limit=1)\nfor r in results:\n    print(f\"Item: {r.key}; Score ({r.score})\")\n    print(f\"Memory: {r.value['memory']}\")\n    print(f\"Emotion: {r.value['emotional_context']}\\n\")\n\nprint(\"Expect random lower score (ravioli not indexed)\")\nresults = store.search((\"user_123\", \"memories\"), query=\"ravioli\", limit=1)\nfor r in results:\n    print(f\"Item: {r.key}; Score ({r.score})\")\n    print(f\"Memory: {r.value['memory']}\")\n    print(f\"Emotion: {r.value['emotional_context']}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 저장 시간에 필드 재정의\n저장소의 기본 구성에 관계없이 `put(..., index=[...fields])`를 사용하여 특정 메모리를 저장할 때 임베드할 필드를 재정의할 수 있습니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "store = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n        \"fields\": [\"memory\"],\n    }  # 기본적으로 memory 필드를 임베드\n)\n\n# 기본 인덱싱으로 하나의 메모리 저장\nstore.put(\n    (\"user_123\", \"memories\"),\n    \"mem1\",\n    {\"memory\": \"I love spicy food\", \"context\": \"At a Thai restaurant\"},\n)\n\n# 임베드할 필드를 재정의하여 다른 메모리 저장\nstore.put(\n    (\"user_123\", \"memories\"),\n    \"mem2\",\n    {\"memory\": \"The restaurant was too loud\", \"context\": \"Dinner at an Italian place\"},\n    index=[\"context\"],  # 재정의: context만 임베드\n)\n\n# 음식에 대한 검색 - mem1과 일치 (기본 필드 사용)\nprint(\"Expect mem1\")\nresults = store.search(\n    (\"user_123\", \"memories\"), query=\"what food do they like\", limit=1\n)\nfor r in results:\n    print(f\"Item: {r.key}; Score ({r.score})\")\n    print(f\"Memory: {r.value['memory']}\")\n    print(f\"Context: {r.value['context']}\\n\")\n\n# 레스토랑 분위기에 대한 검색 - mem2와 일치 (재정의된 필드 사용)\nprint(\"Expect mem2\")\nresults = store.search(\n    (\"user_123\", \"memories\"), query=\"restaurant environment\", limit=1\n)\nfor r in results:\n    print(f\"Item: {r.key}; Score ({r.score})\")\n    print(f\"Memory: {r.value['memory']}\")\n    print(f\"Context: {r.value['context']}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 특정 메모리에 대한 인덱싱 비활성화\n\n일부 메모리는 내용으로 검색할 수 없어야 합니다. `put(..., index=False)`를 사용하여 이러한 메모리를 저장하면서 인덱싱을 비활성화할 수 있습니다. 예제:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "store = InMemoryStore(index={\"embed\": embeddings, \"dims\": 1536, \"fields\": [\"memory\"]})\n\n# 일반 인덱스된 메모리 저장\nstore.put(\n    (\"user_123\", \"memories\"),\n    \"mem1\",\n    {\"memory\": \"I love chocolate ice cream\", \"type\": \"preference\"},\n)\n\n# 인덱싱 없이 시스템 메모리 저장\nstore.put(\n    (\"user_123\", \"memories\"),\n    \"mem2\",\n    {\"memory\": \"User completed onboarding\", \"type\": \"system\"},\n    index=False,  # 인덱싱 완전히 비활성화\n)\n\n# 음식 선호도에 대한 검색 - mem1 찾기\nprint(\"Expect mem1\")\nresults = store.search((\"user_123\", \"memories\"), query=\"what food preferences\", limit=1)\nfor r in results:\n    print(f\"Item: {r.key}; Score ({r.score})\")\n    print(f\"Memory: {r.value['memory']}\")\n    print(f\"Type: {r.value['type']}\\n\")\n\n# 온보딩에 대한 검색 - mem2를 찾지 못함 (인덱스되지 않음)\nprint(\"Expect low score (mem2 not indexed)\")\nresults = store.search((\"user_123\", \"memories\"), query=\"onboarding status\", limit=1)\nfor r in results:\n    print(f\"Item: {r.key}; Score ({r.score})\")\n    print(f\"Memory: {r.value['memory']}\")\n    print(f\"Type: {r.value['type']}\\n\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}